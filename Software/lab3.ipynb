{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vNM0gRKBf-lr",
        "NMqNY8rSgHPc",
        "L9S1QTKdmTtq",
        "ZPkqzRxwWIvZ",
        "s6pADlxgWBvi",
        "zbiLc0hWgLHN",
        "p4yhTSDbgQKU",
        "Y8n9z8mEGUeo",
        "S1FZF3jQGZzw",
        "7hJimk04hx6F",
        "BXdZtX1ih1M1",
        "ZP7Zvu1EqHJR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Setup**"
      ],
      "metadata": {
        "id": "vNM0gRKBf-lr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4T-iZMy5i6KN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79f57f4-3bfd-4e25-a5e4-691a5e571bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 10.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 170kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.85MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 5.52MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import easydict\n",
        "from torchsummary import summary\n",
        "from google.colab import drive\n",
        "from copy import deepcopy\n",
        "import json\n",
        "\n",
        "# argument parser\n",
        "import easydict\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"lr\": 0.01,\n",
        "})\n",
        "# Hyper Parameters\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = args.epochs\n",
        "batch_size = args.batch_size\n",
        "learning_rate = args.lr\n",
        "\n",
        "# MNIST Dataset (Images and Labels)\n",
        "train_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "test_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "\n",
        "# Dataset Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_set,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_set,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Net**"
      ],
      "metadata": {
        "id": "NMqNY8rSgHPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original net"
      ],
      "metadata": {
        "id": "R1JKSoKumP6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConvNet(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MyConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.lin2  = nn.Linear(7*7*32, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "    # Export weights as a json file\n",
        "    def save_weights(self, path):\n",
        "        #torch.save(self.state_dict(), path)\n",
        "        stateDictionary = self.state_dict()\n",
        "        stateDictionary_export = {}\n",
        "        for name,_ in model.named_parameters():\n",
        "            layer_stateDictionary = stateDictionary[name]\n",
        "            #print(\"Orig size: \", layer_stateDictionary.size())\n",
        "            layer_stateDictionary_listView = layer_stateDictionary.tolist()\n",
        "            #print(\"List size: \", len(layer_stateDictionary_listView))\n",
        "            stateDictionary_export[name] = layer_stateDictionary_listView\n",
        "\n",
        "        # Export weights as a json file\n",
        "        with open(path, \"w\") as outfile:\n",
        "            json.dump(stateDictionary_export, outfile)\n",
        "\n",
        "model = MyConvNet(args)\n",
        "model = model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion=criterion.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 5e-4, momentum=0.9)"
      ],
      "metadata": {
        "id": "wsW5cUttmXzK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruned Net"
      ],
      "metadata": {
        "id": "L9S1QTKdmTtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConvNet_pruned(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        numChannels_conv1_output = 12\n",
        "        numChannels_conv2_output = 11\n",
        "\n",
        "        super(MyConvNet_pruned, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, numChannels_conv1_output, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(numChannels_conv1_output, numChannels_conv2_output, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.lin2  = nn.Linear(7*7*numChannels_conv2_output, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model_pruned = MyConvNet_pruned(args)\n",
        "model_pruned = model_pruned.cuda()"
      ],
      "metadata": {
        "id": "2NG4GShUmaoD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initial model summary\")\n",
        "summary(model, (1, 28, 28))\n",
        "\n",
        "print(\"Pruned model summary\")\n",
        "summary(model_pruned, (1, 28, 28))"
      ],
      "metadata": {
        "id": "WL2B0QYb-QHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e600d0ea-16ab-4ed2-e67d-a9c29732ada8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial model summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,608\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]          15,680\n",
            "================================================================\n",
            "Total params: 20,432\n",
            "Trainable params: 20,432\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n",
            "Pruned model summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 12, 28, 28]             108\n",
            "              ReLU-2           [-1, 12, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 12, 14, 14]               0\n",
            "            Conv2d-4           [-1, 11, 14, 14]           1,188\n",
            "              ReLU-5           [-1, 11, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 11, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]           5,390\n",
            "================================================================\n",
            "Total params: 6,686\n",
            "Trainable params: 6,686\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.20\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.23\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "CmYFP9LmHvcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Functions"
      ],
      "metadata": {
        "id": "ZPkqzRxwWIvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_modelAccuracy(model, dataset_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in dataset_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return (100 * correct / total)\n",
        "\n",
        "\n",
        "def loadModel():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    !cp \"/content/drive/My Drive/saved_myconvnet_lab4.pt\" \"./pretrainedModel\"\n",
        "\n",
        "    drive.flush_and_unmount()\n",
        "\n",
        "    global load_model\n",
        "    load_model = MyConvNet(args)\n",
        "    load_model.load_state_dict(torch.load('./pretrainedModel'))\n",
        "\n",
        "    load_model = load_model.cuda()\n",
        "    load_model.eval()\n",
        "\n",
        "\n",
        "def pruneModel(module, dim, amount):\n",
        "    # dim = 0 --> channel pruning\n",
        "    # dim = 2 --> filter row pruning\n",
        "    # dim = 3 --> filter column pruning\n",
        "\n",
        "    prune.ln_structured(module, name=\"weight\", amount=amount,  n=1, dim=dim)\n",
        "    #print(\"Post pruning\")\n",
        "    #print(list(module.named_parameters()))\n",
        "    #print(module.weight)\n",
        "\n",
        "def print_testAccuracy(test_loader):\n",
        "    accuracy = get_modelAccuracy(test_loader).data.item()\n",
        "    print('Accuracy for test images: % d %%' % accuracy)\n",
        "\n",
        "def as_list(x):\n",
        "    if type(x) is list:\n",
        "        return x\n",
        "    else:\n",
        "        return [x]\n",
        "\n",
        "def evaluateModelAccuracy(model, stateDictionary, test_loader):\n",
        "    # Load state dict\n",
        "    model.load_state_dict(stateDictionary)\n",
        "    # Test the Model\n",
        "    accuracy = get_modelAccuracy(model, test_loader)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "9Lp3dYKIH5gZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization Functions"
      ],
      "metadata": {
        "id": "s6pADlxgWBvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def USquantize(x,bits=4):\n",
        "  # Uniform symetric quantizer that\n",
        "  # quantize x into sf * qx\n",
        "  # sf: scaling factor\n",
        "  # qx: integer in range [-2^(bits-1)+1, 2^(bits-1)-1]\n",
        "  # note: only 2^bits - 1 different values can be represented, bits >= 2\n",
        "\n",
        "  max_value = torch.max(torch.abs(x))\n",
        "  sf = max_value / (2**(bits-1) - 1)\n",
        "  qx = torch.round(x/sf)\n",
        "  qx = torch.clip(qx,min=-2**(bits-1) + 1,max=2**(bits-1) - 1)\n",
        "  dqx = qx * sf\n",
        "  return dqx\n",
        "\n",
        "def UASquantize(x,bits=4):\n",
        "  # Uniform Asymetric quantizer that\n",
        "  # quantize x into min_value + sf * qx\n",
        "  # sf: scaling factor\n",
        "  # qx: integer in range [0,2^bits-1]\n",
        "  # note: 2^bits different values can be represented, bits >= 1\n",
        "\n",
        "  max_value = torch.max(x)\n",
        "  min_value = torch.min(x)\n",
        "  sf = (max_value - min_value) / (2**bits - 1)\n",
        "  qx = torch.round((x-min_value)/sf)\n",
        "  qx = torch.clip(qx,min=0,max=2**(bits) - 1)\n",
        "  dqx = min_value + qx * sf\n",
        "  return dqx\n",
        "\n",
        "def UASquantizeMinOffset(x,p,bits=4):\n",
        "  # Uniform Asymetric quantizer that\n",
        "  # quantize x into min_value + sf * qx\n",
        "  # sf: scaling factor\n",
        "  # p: percent offset away from the native min. Can be negative or positive\n",
        "  # qx: integer in range [0,2^bits-1]\n",
        "  # note: 2^bits different values can be represented, bits >= 1\n",
        "\n",
        "  max_value = torch.max(x)\n",
        "  min_value = torch.min(x)\n",
        "  # update min_value to be set by p\n",
        "  min_value = min_value * p\n",
        "  sf = (max_value - min_value) / (2**bits - 1)\n",
        "  qx = torch.round((x-min_value)/sf)\n",
        "  qx = torch.clip(qx,min=0,max=2**(bits) - 1)\n",
        "  dqx = min_value + qx * sf\n",
        "  return dqx\n",
        "\n",
        "\n",
        "def USquantize_clipped(x,bits=4,quantile=0.999):\n",
        "\n",
        "  # Uniform Symetric quantizer with clipped representation range\n",
        "  # range covers 'quantile' percent of FP32 x\n",
        "  max_value = torch.quantile(x, 1 - 0.5 * (1 - quantile))  # Upper quantile\n",
        "  min_value = torch.quantile(x, 0.5 * (1 - quantile))      # Lower quantile\n",
        "  quantile_max = max(abs(max_value), abs(min_value))  # Ensure symmetry around zero\n",
        "  sf = quantile_max / (2 ** (bits - 1) - 1)\n",
        "  qx = torch.round(x/sf)\n",
        "  qx = torch.clip(qx,min=-2**(bits-1) + 1,max=2**(bits-1) - 1)\n",
        "  dqx = qx * sf\n",
        "  return dqx\n",
        "\n",
        "def UASquantize_clipped(x,bits=4,quantile=0.999):\n",
        "\n",
        "  # Uniform Asymetric quantizer with clipped representation range\n",
        "  # range covers 'quantile' percent of FP32 x\n",
        "\n",
        "  max_value = torch.quantile(x,1-0.5*(1-quantile))\n",
        "  min_value = torch.quantile(x,0.5*(1-quantile))\n",
        "  sf = (max_value - min_value) / (2**bits - 1)\n",
        "  qx = torch.round((x-min_value)/sf)\n",
        "  qx = torch.clip(qx,min=0,max=2**(bits) - 1)\n",
        "  dqx = min_value + qx * sf\n",
        "  return dqx\n",
        "\n",
        "def quantize(scheme, values, bits=4):\n",
        "    if scheme == \"FullRange_Symmetric\":\n",
        "        values_quantized = USquantize(values, bits=bits)\n",
        "    elif scheme == \"FullRange_Asymmetric\":\n",
        "        values_quantized = UASquantize(values, bits=bits)\n",
        "    elif scheme == \"OptimalRange_Symmetric\":\n",
        "        values_quantized = USquantize_clipped(values, bits=bits)\n",
        "    elif scheme == \"OptimalRange_Asymmetric\":\n",
        "        values_quantized = UASquantize_clipped(values, bits=bits)\n",
        "    else:\n",
        "        print(\"ERROR:: Unexpected quantization scheme: \", scheme)\n",
        "\n",
        "    return values_quantized"
      ],
      "metadata": {
        "id": "UqboujYLWAVe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "zbiLc0hWgLHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Training started\")\n",
        "# Training the Model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        L1norm = model.parameters()\n",
        "        arr = []\n",
        "        for name,param in model.named_parameters():\n",
        "          if 'weight' in name.split('.'):\n",
        "            arr.append(param)\n",
        "        L1loss = 0\n",
        "        for Losstmp in arr:\n",
        "          L1loss = L1loss+Losstmp.abs().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 600 == 0:\n",
        "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                    % (epoch + 1, num_epochs, i + 1,\n",
        "                       len(train_set) // batch_size, loss.data.item()))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "for images, labels in test_loader:\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    outputs = model(images)\n",
        "    testloss = criterion(outputs, labels)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print('Accuracy for test images: % d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xxlXQl7jCob",
        "outputId": "a273f375-a0c2-4333-ee70-8ad374bb1776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Training started\n",
            "Epoch: [ 1/ 10], Step: [ 600/ 1875], Loss: 0.5599\n",
            "Epoch: [ 1/ 10], Step: [ 1200/ 1875], Loss: 0.2484\n",
            "Epoch: [ 1/ 10], Step: [ 1800/ 1875], Loss: 0.3158\n",
            "Epoch: [ 2/ 10], Step: [ 600/ 1875], Loss: 0.5228\n",
            "Epoch: [ 2/ 10], Step: [ 1200/ 1875], Loss: 0.2680\n",
            "Epoch: [ 2/ 10], Step: [ 1800/ 1875], Loss: 0.4812\n",
            "Epoch: [ 3/ 10], Step: [ 600/ 1875], Loss: 0.2206\n",
            "Epoch: [ 3/ 10], Step: [ 1200/ 1875], Loss: 0.3994\n",
            "Epoch: [ 3/ 10], Step: [ 1800/ 1875], Loss: 0.1739\n",
            "Epoch: [ 4/ 10], Step: [ 600/ 1875], Loss: 0.2520\n",
            "Epoch: [ 4/ 10], Step: [ 1200/ 1875], Loss: 0.4312\n",
            "Epoch: [ 4/ 10], Step: [ 1800/ 1875], Loss: 0.2988\n",
            "Epoch: [ 5/ 10], Step: [ 600/ 1875], Loss: 0.2379\n",
            "Epoch: [ 5/ 10], Step: [ 1200/ 1875], Loss: 0.2737\n",
            "Epoch: [ 5/ 10], Step: [ 1800/ 1875], Loss: 0.1225\n",
            "Epoch: [ 6/ 10], Step: [ 600/ 1875], Loss: 0.2011\n",
            "Epoch: [ 6/ 10], Step: [ 1200/ 1875], Loss: 0.3740\n",
            "Epoch: [ 6/ 10], Step: [ 1800/ 1875], Loss: 0.1236\n",
            "Epoch: [ 7/ 10], Step: [ 600/ 1875], Loss: 0.2469\n",
            "Epoch: [ 7/ 10], Step: [ 1200/ 1875], Loss: 0.1464\n",
            "Epoch: [ 7/ 10], Step: [ 1800/ 1875], Loss: 0.3244\n",
            "Epoch: [ 8/ 10], Step: [ 600/ 1875], Loss: 0.1458\n",
            "Epoch: [ 8/ 10], Step: [ 1200/ 1875], Loss: 0.2406\n",
            "Epoch: [ 8/ 10], Step: [ 1800/ 1875], Loss: 0.0811\n",
            "Epoch: [ 9/ 10], Step: [ 600/ 1875], Loss: 0.1800\n",
            "Epoch: [ 9/ 10], Step: [ 1200/ 1875], Loss: 0.0836\n",
            "Epoch: [ 9/ 10], Step: [ 1800/ 1875], Loss: 0.3386\n",
            "Epoch: [ 10/ 10], Step: [ 600/ 1875], Loss: 0.1276\n",
            "Epoch: [ 10/ 10], Step: [ 1200/ 1875], Loss: 0.1386\n",
            "Epoch: [ 10/ 10], Step: [ 1800/ 1875], Loss: 0.0374\n",
            "Accuracy for test images:  90 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save Model**"
      ],
      "metadata": {
        "id": "p4yhTSDbgQKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"./saved_myconvnet_lab4.pt\"\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp \"./saved_myconvnet_lab4.pt\" \"/content/drive/My Drive\"\n",
        "\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "i6dFkcnKQBpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Model**"
      ],
      "metadata": {
        "id": "Y8n9z8mEGUeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_model = None\n",
        "loadModel()"
      ],
      "metadata": {
        "id": "QN2ge-5n4yI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a655d3-36c4-4db4-b97e-5d16b593cb2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-8ccbc4905191>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  load_model.load_state_dict(torch.load('./pretrainedModel'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Validate pruning implementation\n",
        "# loadModel()\n",
        "# module = load_model.conv1\n",
        "# weights = module.weight.data\n",
        "# size = module.weight.size()\n",
        "# dims = [i for i in range(len(size))]\n",
        "\n",
        "# sum_index = {}\n",
        "# for i0 in range(size[3]):\n",
        "#     innerSum = 0\n",
        "#     for i1 in range(size[1]):\n",
        "#         for i2 in range(size[0]):\n",
        "#             for i3 in range(size[2]):\n",
        "#                 innerSum += abs(weights[i2][i1][i3][i0].item())\n",
        "\n",
        "#     #print(i0, \": \", innerSum)\n",
        "#     sum_index[innerSum] = i0\n",
        "\n",
        "# sortedSums = sorted(sum_index.keys())\n",
        "# print(\"Total #sums = \", len(sortedSums))\n",
        "# for sum in sortedSums:\n",
        "#     index = sum_index[sum]\n",
        "#     print(index, \" : \", sum)\n"
      ],
      "metadata": {
        "id": "kTuhHIbG9dZ_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test**"
      ],
      "metadata": {
        "id": "S1FZF3jQGZzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = get_modelAccuracy(test_loader).data.item()\n",
        "print('Accuracy for test images: % d %%' % accuracy)\n",
        "#summary(model, (1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57mk9dpwGh6f",
        "outputId": "6b6e0eef-87b6-405e-8c0d-7cba57c3e305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for test images:  90 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prune**"
      ],
      "metadata": {
        "id": "yhR-v6VLst6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testbench"
      ],
      "metadata": {
        "id": "7hJimk04hx6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testVectorsConv1_dim_amount = [\n",
        "    [0, 1/12],\n",
        "    [2, 1/3],\n",
        "    [3, 1/3]\n",
        "]\n",
        "\n",
        "testVectorsConv2_dim_amount = [\n",
        "    [0, 1/15],\n",
        "    [2, 1/3],\n",
        "    [3, 1/3]\n",
        "]\n",
        "\n",
        "testVectorsOfLayer = {\n",
        "    \"conv1\" : testVectorsConv1_dim_amount,\n",
        "    \"conv2\" : testVectorsConv2_dim_amount\n",
        "}\n",
        "\n",
        "for layer in [\"conv2\", \"conv1\"]:\n",
        "    print(\"Pruning layer: \", layer)\n",
        "\n",
        "    for dim, amount in testVectorsOfLayer[layer]:\n",
        "        loadModel()\n",
        "        #pruneModel(module=load_model.conv2, dim=0, amount=5/32)\n",
        "        #pruneModel(module=load_model.conv1, dim=0, amount=1/16)\n",
        "        #pruneModel(module=load_model.conv2, dim=0, amount=12/32)\n",
        "        pruneModel(module=load_model.conv1, dim=0, amount=4/16)\n",
        "        pruneModel(module=load_model.conv2, dim=0, amount=17/32)\n",
        "        accuracy_baseline = get_modelAccuracy(load_model, test_loader)\n",
        "\n",
        "        if layer == \"conv1\":\n",
        "            module = load_model.conv1\n",
        "        elif layer == \"conv2\":\n",
        "            module = load_model.conv2\n",
        "\n",
        "        pruneModel(module=module, dim=dim, amount=amount)\n",
        "        print(\"Pruned dim = \", dim, \", amount = \", amount)\n",
        "        accuracy_pruned = get_modelAccuracy(load_model, test_loader)\n",
        "        print(\"Accuracy for test images: %.2f%% --> %.2f%%\" % (accuracy_baseline, accuracy_pruned))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Mac2RPcJGy",
        "outputId": "11426458-4139-470e-fd9b-cd5cc1dbe587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruning layer:  conv2\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-9238bd9a0fc3>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  load_model.load_state_dict(torch.load('./pretrainedModel'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned dim =  0 , amount =  0.06666666666666667\n",
            "Accuracy for test images: 86.70% --> 85.54%\n",
            "Mounted at /content/drive\n",
            "Pruned dim =  2 , amount =  0.3333333333333333\n",
            "Accuracy for test images: 86.70% --> 80.45%\n",
            "Mounted at /content/drive\n",
            "Pruned dim =  3 , amount =  0.3333333333333333\n",
            "Accuracy for test images: 86.70% --> 81.75%\n",
            "Pruning layer:  conv1\n",
            "Mounted at /content/drive\n",
            "Pruned dim =  0 , amount =  0.08333333333333333\n",
            "Accuracy for test images: 86.70% --> 83.44%\n",
            "Mounted at /content/drive\n",
            "Pruned dim =  2 , amount =  0.3333333333333333\n",
            "Accuracy for test images: 86.70% --> 75.55%\n",
            "Mounted at /content/drive\n",
            "Pruned dim =  3 , amount =  0.3333333333333333\n",
            "Accuracy for test images: 86.70% --> 65.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Pruned Model"
      ],
      "metadata": {
        "id": "BXdZtX1ih1M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loadModel()\n",
        "accuracy_baseline = get_modelAccuracy(test_loader).data.item()\n",
        "print(\"Initial model summary\")\n",
        "summary(load_model, (1, 28, 28))\n",
        "\n",
        "pruneModel(module=load_model.conv1, dim=0, amount=4/16)\n",
        "pruneModel(module=load_model.conv2, dim=0, amount=17/32)\n",
        "accuracy_pruned = get_modelAccuracy(test_loader).data.item()\n",
        "print(\"Accuracy for test images: %d%% --> %d%%\" % (accuracy_baseline, accuracy_pruned))\n",
        "\n",
        "prune.remove(module=load_model.conv1, name='weight')\n",
        "prune.remove(module=load_model.conv2, name='weight')\n",
        "print(\"Pruned model summary\")\n",
        "summary(load_model, (1, 28, 28))\n"
      ],
      "metadata": {
        "id": "mPfEddWkh7zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da00528b-0c1a-4048-ca44-d6a0e798791d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-1f8a9b2ac6e7>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  load_model.load_state_dict(torch.load('./pretrainedModel'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial model summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,608\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]          15,680\n",
            "================================================================\n",
            "Total params: 20,432\n",
            "Trainable params: 20,432\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n",
            "Accuracy for test images: 90% --> 86%\n",
            "Pruned model summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,608\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]          15,680\n",
            "================================================================\n",
            "Total params: 20,432\n",
            "Trainable params: 20,432\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,608\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]          15,680\n",
            "================================================================\n",
            "Total params: 20,432\n",
            "Trainable params: 20,432\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantize**"
      ],
      "metadata": {
        "id": "D5EnMoS7XSi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight Quantization"
      ],
      "metadata": {
        "id": "ftgvBqyW0qv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_model = None\n",
        "\n",
        "def quantize_weights(load_model):\n",
        "    global q_model\n",
        "    q_model = deepcopy(load_model)\n",
        "    sd = load_model.state_dict()\n",
        "    q_sd = q_model.state_dict()\n",
        "    cuda_available =torch.cuda.is_available()\n",
        "\n",
        "    # Switches for quantization scheme:\n",
        "                #   FullRange_Symmetric, FullRange_Asymmetric, OptimalRange_Symmetric\n",
        "    config = {\n",
        "        \"conv1.weight\" : {\n",
        "            \"enable_quantization\" : True,\n",
        "            #\"numBits\" : [4, 8, 12, 16, 32],\n",
        "            \"numBits\" : 8,\n",
        "            #\"quantizationSchemes\" : [\"FullRange_Symmetric\", \"FullRange_Asymmetric\", \"OptimalRange_Symmetric\", \"OptimalRange_Asymmetric\"]\n",
        "            \"quantizationSchemes\" : \"FullRange_Asymmetric\"\n",
        "        },\n",
        "\n",
        "        \"conv2.weight\" : {\n",
        "            \"enable_quantization\" : True,\n",
        "            #\"numBits\" : [4, 8, 12, 16, 32],\n",
        "            \"numBits\" : 4,\n",
        "            #\"quantizationSchemes\" : [\"FullRange_Symmetric\", \"FullRange_Asymmetric\", \"OptimalRange_Symmetric\", \"OptimalRange_Asymmetric\"]\n",
        "            \"quantizationSchemes\" : \"OptimalRange_Symmetric\"\n",
        "        },\n",
        "\n",
        "        \"lin2.weight\" : {\n",
        "            \"enable_quantization\" : True,\n",
        "            \"numBits\" : 4,\n",
        "            #\"quantizationSchemes\" : [\"FullRange_Symmetric\", \"FullRange_Asymmetric\", \"OptimalRange_Symmetric\", \"OptimalRange_Asymmetric\"]\n",
        "            \"quantizationSchemes\" : \"FullRange_Asymmetric\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Iterate over layers\n",
        "    for name in [\"lin2.weight\", \"conv2.weight\", \"conv1.weight\"]:\n",
        "        print()\n",
        "        if (not config[name][\"enable_quantization\"]):\n",
        "            continue\n",
        "        print(\"Quantizing \", name)\n",
        "        numBits             = as_list(config[name][\"numBits\"])\n",
        "        quantizationSchemes = as_list(config[name][\"quantizationSchemes\"])\n",
        "\n",
        "        # Iterate over quantization schemes:\n",
        "        for scheme in quantizationSchemes:\n",
        "            print(\"  \", scheme)\n",
        "\n",
        "            # Iterate over number of bits\n",
        "            for n_bits in numBits:\n",
        "                print(\"    \", n_bits, \" bits\")\n",
        "                q_sd[name] = quantize(scheme, sd[name], bits=n_bits)\n",
        "\n",
        "                if len(numBits) > 1:\n",
        "                    accuracy = evaluateModelAccuracy(q_model, q_sd, test_loader)\n",
        "                    print('        Test accuracy: %.2f %%' % (accuracy))\n",
        "\n",
        "            if len(quantizationSchemes) > 1:\n",
        "                accuracy = evaluateModelAccuracy(q_model, q_sd, test_loader)\n",
        "                print('      Test accuracy: %.2f %%' % (accuracy))\n",
        "\n",
        "    accuracy = evaluateModelAccuracy(q_model, q_sd, test_loader)\n",
        "    print('Test accuracy: %.2f %%' % (accuracy))\n",
        "\n",
        "load_model = None\n",
        "for i in range(2):\n",
        "    loadModel()\n",
        "    quantize_weights(load_model)\n",
        "\n",
        "    # Save weights after quantization\n",
        "    q_model.save_weights(\"./lab3_quantized\")\n",
        "\n",
        "    pruneModel(module=q_model.conv2, dim=0, amount=21/32)\n",
        "    pruneModel(module=q_model.conv1, dim=0, amount=4/16)\n",
        "\n",
        "    # Make the pruning 'permanent' so it can be saved\n",
        "    prune.remove(q_model.conv2, 'weight')\n",
        "    prune.remove(q_model.conv1, 'weight')\n",
        "    q_model.save_weights(\"./lab3_quantized_pruned\")\n",
        "\n",
        "    accuracy_pruned = get_modelAccuracy(q_model, test_loader)\n",
        "    print('Test accuracy after pruning: %.2f %%' % (accuracy_pruned))\n",
        "\n",
        "    if i == 0:\n",
        "        pruneModel(module=q_model.conv2, dim=0, amount=1/11)\n",
        "        accuracy_pruned = get_modelAccuracy(q_model, test_loader)\n",
        "        print('Test accuracy after further pruning conv2: %.2f %%' % (accuracy_pruned))\n",
        "    elif i == 1:\n",
        "        pruneModel(module=q_model.conv1, dim=0, amount=1/12)\n",
        "        accuracy_pruned = get_modelAccuracy(q_model, test_loader)\n",
        "        print('Test accuracy after further pruning conv1: %.2f %%' % (accuracy_pruned))\n",
        "\n",
        "# Save models to Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp \"./lab3_quantized\" \"/content/drive/My Drive\"\n",
        "!cp \"./lab3_quantized_pruned\" \"/content/drive/My Drive\"\n",
        "\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OAlP-kuXQkF",
        "outputId": "5e3cfab2-a5a2-44e6-e8b0-56e774dec745"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-8ccbc4905191>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  load_model.load_state_dict(torch.load('./pretrainedModel'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Quantizing  lin2.weight\n",
            "   FullRange_Asymmetric\n",
            "     4  bits\n",
            "\n",
            "Quantizing  conv2.weight\n",
            "   OptimalRange_Symmetric\n",
            "     4  bits\n",
            "\n",
            "Quantizing  conv1.weight\n",
            "   FullRange_Asymmetric\n",
            "     8  bits\n",
            "Test accuracy: 90.08 %\n",
            "Test accuracy after pruning: 80.13 %\n",
            "Test accuracy after further pruning conv2: 80.13 %\n",
            "Mounted at /content/drive\n",
            "\n",
            "Quantizing  lin2.weight\n",
            "   FullRange_Asymmetric\n",
            "     4  bits\n",
            "\n",
            "Quantizing  conv2.weight\n",
            "   OptimalRange_Symmetric\n",
            "     4  bits\n",
            "\n",
            "Quantizing  conv1.weight\n",
            "   FullRange_Asymmetric\n",
            "     8  bits\n",
            "Test accuracy: 90.08 %\n",
            "Test accuracy after pruning: 80.13 %\n",
            "Test accuracy after further pruning conv1: 80.13 %\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation and Weight Quantization"
      ],
      "metadata": {
        "id": "ZP7Zvu1EqHJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ACTIVATION AND Wright Quantization\n",
        "# Deepcopy the model for quantization\n",
        "quantized_model = deepcopy(load_model)\n",
        "\n",
        "# Simulate quantization on the deepcopy (for evaluation)\n",
        "def apply_quantization_to_model(model, bits=4):\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Quantize the weights of the model\n",
        "    with torch.no_grad():  # No need to track gradients during quantization\n",
        "        # Quantize the weights of each layer\n",
        "        model.conv1.weight.data = UASquantize(model.conv1.weight.data, bits)\n",
        "        model.conv2.weight.data = UASquantize(model.conv2.weight.data, bits)\n",
        "        model.lin2.weight.data = UASquantize(model.lin2.weight.data, bits)\n",
        "\n",
        "    # Apply quantization to activations after each layer\n",
        "    def quantized_forward(x):\n",
        "        x = model.conv1(x)\n",
        "        x = model.act1(x)\n",
        "        x = UASquantize(x, bits)  # Quantize activations after conv1\n",
        "        x = model.pool1(x)\n",
        "\n",
        "        x = model.conv2(x)\n",
        "        x = model.act2(x)\n",
        "        x = UASquantize(x, bits)  # Quantize activations after conv2\n",
        "        x = model.pool2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = model.lin2(x)\n",
        "        x = UASquantize(x, bits)  # Quantize activations after FC layer\n",
        "        return x\n",
        "\n",
        "    # Replace the model's forward pass with the quantized forward pass\n",
        "    model.forward = quantized_forward\n",
        "\n",
        "for n_bits in (4, 8, 12, 16):\n",
        "    # Apply quantization to the copied model\n",
        "    apply_quantization_to_model(quantized_model, bits=n_bits)\n",
        "\n",
        "    # Now you can evaluate both models\n",
        "    # Example: Evaluate the original model and the quantized model on some test data\n",
        "    def evaluate_model(model, test_loader):\n",
        "        model.eval()  # Set to evaluation mode\n",
        "\n",
        "        accuracy = get_modelAccuracy(model, test_loader)\n",
        "        return accuracy\n",
        "\n",
        "    # Assuming `test_loader` is your DataLoader for test data\n",
        "    original_accuracy = evaluate_model(load_model, test_loader)\n",
        "    quantized_accuracy = evaluate_model(quantized_model, test_loader)\n",
        "\n",
        "    print(\"With %d-bit quantization:\" % (n_bits))\n",
        "    print(f\"  Original Model Accuracy: {original_accuracy:.2f}%\")\n",
        "    print(f\"  Quantized Model Accuracy: {quantized_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdBbxZvIqEPW",
        "outputId": "7b75a648-8d93-4632-c5ac-efdd87ebb6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With 4-bit quantization:\n",
            "  Original Model Accuracy: 90.39%\n",
            "  Quantized Model Accuracy: 88.92%\n",
            "With 8-bit quantization:\n",
            "  Original Model Accuracy: 90.39%\n",
            "  Quantized Model Accuracy: 89.79%\n",
            "With 12-bit quantization:\n",
            "  Original Model Accuracy: 90.39%\n",
            "  Quantized Model Accuracy: 89.75%\n",
            "With 16-bit quantization:\n",
            "  Original Model Accuracy: 90.39%\n",
            "  Quantized Model Accuracy: 89.75%\n"
          ]
        }
      ]
    }
  ]
}