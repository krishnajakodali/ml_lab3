{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vNM0gRKBf-lr",
        "NMqNY8rSgHPc",
        "zbiLc0hWgLHN",
        "p4yhTSDbgQKU",
        "Y8n9z8mEGUeo",
        "S1FZF3jQGZzw"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnajakodali/ml_lab3/blob/main/lab3_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Setup**"
      ],
      "metadata": {
        "id": "vNM0gRKBf-lr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T-iZMy5i6KN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4574918b-b5b6-45b9-8960-8a7838379d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 208kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.85MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 23.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import easydict\n",
        "from torchsummary import summary\n",
        "from google.colab import drive\n",
        "\n",
        "# argument parser\n",
        "import easydict\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"lr\": 0.01,\n",
        "})\n",
        "# Hyper Parameters\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = args.epochs\n",
        "batch_size = args.batch_size\n",
        "learning_rate = args.lr\n",
        "\n",
        "# MNIST Dataset (Images and Labels)\n",
        "train_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "test_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "\n",
        "# Dataset Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_set,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_set,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Net**"
      ],
      "metadata": {
        "id": "NMqNY8rSgHPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original net"
      ],
      "metadata": {
        "id": "R1JKSoKumP6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConvNet(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MyConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.lin2  = nn.Linear(7*7*32, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model = MyConvNet(args)\n",
        "model = model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion=criterion.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 5e-4, momentum=0.9)"
      ],
      "metadata": {
        "id": "wsW5cUttmXzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruned Net"
      ],
      "metadata": {
        "id": "L9S1QTKdmTtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConvNet_pruned(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        numChannels_conv1_output = 12\n",
        "        numChannels_conv2_output = 15\n",
        "\n",
        "        super(MyConvNet_pruned, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, numChannels_conv1_output, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(numChannels_conv1_output, numChannels_conv2_output, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.lin2  = nn.Linear(7*7*numChannels_conv2_output, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model_pruned = MyConvNet_pruned(args)\n",
        "model_pruned = model_pruned.cuda()"
      ],
      "metadata": {
        "id": "2NG4GShUmaoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initial model summary\")\n",
        "summary(model, (1, 28, 28))\n",
        "\n",
        "print(\"Pruned model summary\")\n",
        "summary(model_pruned, (1, 28, 28))"
      ],
      "metadata": {
        "id": "WL2B0QYb-QHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb46de94-37f8-4f03-e601-d65545914488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial model summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,608\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]          15,680\n",
            "================================================================\n",
            "Total params: 20,432\n",
            "Trainable params: 20,432\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n",
            "Pruned model summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 12, 28, 28]             108\n",
            "              ReLU-2           [-1, 12, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 12, 14, 14]               0\n",
            "            Conv2d-4           [-1, 15, 14, 14]           1,620\n",
            "              ReLU-5           [-1, 15, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 15, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]           7,350\n",
            "================================================================\n",
            "Total params: 9,078\n",
            "Trainable params: 9,078\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.21\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.25\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions**"
      ],
      "metadata": {
        "id": "CmYFP9LmHvcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_modelAccuracy(dataset_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in dataset_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = load_model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    return (100 * correct / total)\n",
        "\n",
        "\n",
        "def loadModel():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    !cp \"/content/drive/My Drive/saved_myconvnet_lab4.pt\" \"./pretrainedModel\"\n",
        "\n",
        "    drive.flush_and_unmount()\n",
        "\n",
        "    global load_model\n",
        "    load_model = MyConvNet(args)\n",
        "    load_model.load_state_dict(torch.load('./pretrainedModel'))\n",
        "\n",
        "    load_model = load_model.cuda()\n",
        "    load_model.eval()\n",
        "\n",
        "\n",
        "def pruneModel(module, dim, amount):\n",
        "    # dim = 0 --> channel pruning\n",
        "    # dim = 2 --> filter row pruning\n",
        "    # dim = 3 --> filter column pruning\n",
        "\n",
        "    prune.ln_structured(module, name=\"weight\", amount=amount,  n=1, dim=dim)\n",
        "    #print(\"Post pruning\")\n",
        "    #print(list(module.named_parameters()))\n",
        "    #print(module.weight)\n",
        "\n",
        "def print_testAccuracy(test_loader):\n",
        "    accuracy = get_modelAccuracy(test_loader).data.item()\n",
        "    print('Accuracy for test images: % d %%' % accuracy)"
      ],
      "metadata": {
        "id": "9Lp3dYKIH5gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "zbiLc0hWgLHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Training started\")\n",
        "# Training the Model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        L1norm = model.parameters()\n",
        "        arr = []\n",
        "        for name,param in model.named_parameters():\n",
        "          if 'weight' in name.split('.'):\n",
        "            arr.append(param)\n",
        "        L1loss = 0\n",
        "        for Losstmp in arr:\n",
        "          L1loss = L1loss+Losstmp.abs().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 600 == 0:\n",
        "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                    % (epoch + 1, num_epochs, i + 1,\n",
        "                       len(train_set) // batch_size, loss.data.item()))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "for images, labels in test_loader:\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    outputs = model(images)\n",
        "    testloss = criterion(outputs, labels)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print('Accuracy for test images: % d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xxlXQl7jCob",
        "outputId": "a273f375-a0c2-4333-ee70-8ad374bb1776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Training started\n",
            "Epoch: [ 1/ 10], Step: [ 600/ 1875], Loss: 0.5599\n",
            "Epoch: [ 1/ 10], Step: [ 1200/ 1875], Loss: 0.2484\n",
            "Epoch: [ 1/ 10], Step: [ 1800/ 1875], Loss: 0.3158\n",
            "Epoch: [ 2/ 10], Step: [ 600/ 1875], Loss: 0.5228\n",
            "Epoch: [ 2/ 10], Step: [ 1200/ 1875], Loss: 0.2680\n",
            "Epoch: [ 2/ 10], Step: [ 1800/ 1875], Loss: 0.4812\n",
            "Epoch: [ 3/ 10], Step: [ 600/ 1875], Loss: 0.2206\n",
            "Epoch: [ 3/ 10], Step: [ 1200/ 1875], Loss: 0.3994\n",
            "Epoch: [ 3/ 10], Step: [ 1800/ 1875], Loss: 0.1739\n",
            "Epoch: [ 4/ 10], Step: [ 600/ 1875], Loss: 0.2520\n",
            "Epoch: [ 4/ 10], Step: [ 1200/ 1875], Loss: 0.4312\n",
            "Epoch: [ 4/ 10], Step: [ 1800/ 1875], Loss: 0.2988\n",
            "Epoch: [ 5/ 10], Step: [ 600/ 1875], Loss: 0.2379\n",
            "Epoch: [ 5/ 10], Step: [ 1200/ 1875], Loss: 0.2737\n",
            "Epoch: [ 5/ 10], Step: [ 1800/ 1875], Loss: 0.1225\n",
            "Epoch: [ 6/ 10], Step: [ 600/ 1875], Loss: 0.2011\n",
            "Epoch: [ 6/ 10], Step: [ 1200/ 1875], Loss: 0.3740\n",
            "Epoch: [ 6/ 10], Step: [ 1800/ 1875], Loss: 0.1236\n",
            "Epoch: [ 7/ 10], Step: [ 600/ 1875], Loss: 0.2469\n",
            "Epoch: [ 7/ 10], Step: [ 1200/ 1875], Loss: 0.1464\n",
            "Epoch: [ 7/ 10], Step: [ 1800/ 1875], Loss: 0.3244\n",
            "Epoch: [ 8/ 10], Step: [ 600/ 1875], Loss: 0.1458\n",
            "Epoch: [ 8/ 10], Step: [ 1200/ 1875], Loss: 0.2406\n",
            "Epoch: [ 8/ 10], Step: [ 1800/ 1875], Loss: 0.0811\n",
            "Epoch: [ 9/ 10], Step: [ 600/ 1875], Loss: 0.1800\n",
            "Epoch: [ 9/ 10], Step: [ 1200/ 1875], Loss: 0.0836\n",
            "Epoch: [ 9/ 10], Step: [ 1800/ 1875], Loss: 0.3386\n",
            "Epoch: [ 10/ 10], Step: [ 600/ 1875], Loss: 0.1276\n",
            "Epoch: [ 10/ 10], Step: [ 1200/ 1875], Loss: 0.1386\n",
            "Epoch: [ 10/ 10], Step: [ 1800/ 1875], Loss: 0.0374\n",
            "Accuracy for test images:  90 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save Model**"
      ],
      "metadata": {
        "id": "p4yhTSDbgQKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"./saved_myconvnet_lab4.pt\"\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp \"./saved_myconvnet_lab4.pt\" \"/content/drive/My Drive\"\n",
        "\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "i6dFkcnKQBpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Model**"
      ],
      "metadata": {
        "id": "Y8n9z8mEGUeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "load_model = None"
      ],
      "metadata": {
        "id": "QN2ge-5n4yI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate pruning implementation\n",
        "loadModel()\n",
        "module = load_model.conv1\n",
        "weights = module.weight.data\n",
        "size = module.weight.size()\n",
        "dims = [i for i in range(len(size))]\n",
        "\n",
        "sum_index = {}\n",
        "for i0 in range(size[3]):\n",
        "    innerSum = 0\n",
        "    for i1 in range(size[1]):\n",
        "        for i2 in range(size[0]):\n",
        "            for i3 in range(size[2]):\n",
        "                innerSum += abs(weights[i2][i1][i3][i0].item())\n",
        "\n",
        "    #print(i0, \": \", innerSum)\n",
        "    sum_index[innerSum] = i0\n",
        "\n",
        "sortedSums = sorted(sum_index.keys())\n",
        "print(\"Total #sums = \", len(sortedSums))\n",
        "for sum in sortedSums:\n",
        "    index = sum_index[sum]\n",
        "    print(index, \" : \", sum)\n"
      ],
      "metadata": {
        "id": "kTuhHIbG9dZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test**"
      ],
      "metadata": {
        "id": "S1FZF3jQGZzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = get_modelAccuracy(test_loader).data.item()\n",
        "print('Accuracy for test images: % d %%' % accuracy)\n",
        "#summary(model, (1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57mk9dpwGh6f",
        "outputId": "6b6e0eef-87b6-405e-8c0d-7cba57c3e305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for test images:  90 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prune**"
      ],
      "metadata": {
        "id": "yhR-v6VLst6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testbench"
      ],
      "metadata": {
        "id": "7hJimk04hx6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testVectorsConv1_dim_amount = [\n",
        "    [0, 1/12],\n",
        "    [2, 1/3],\n",
        "    [3, 1/3]\n",
        "]\n",
        "\n",
        "testVectorsConv2_dim_amount = [\n",
        "    [0, 1/15],\n",
        "    [2, 1/3],\n",
        "    [3, 1/3]\n",
        "]\n",
        "\n",
        "testVectorsOfLayer = {\n",
        "    \"conv1\" : testVectorsConv1_dim_amount,\n",
        "    \"conv2\" : testVectorsConv2_dim_amount\n",
        "}\n",
        "\n",
        "for layer in [\"conv2\", \"conv1\"]:\n",
        "    print(\"Pruning layer: \", layer)\n",
        "\n",
        "    for dim, amount in testVectorsOfLayer[layer]:\n",
        "        loadModel()\n",
        "        #pruneModel(module=load_model.conv2, dim=0, amount=5/32)\n",
        "        #pruneModel(module=load_model.conv1, dim=0, amount=1/16)\n",
        "        #pruneModel(module=load_model.conv2, dim=0, amount=12/32)\n",
        "        pruneModel(module=load_model.conv1, dim=0, amount=4/16)\n",
        "        pruneModel(module=load_model.conv2, dim=0, amount=17/32)\n",
        "        accuracy_baseline = get_modelAccuracy(test_loader).data.item()\n",
        "\n",
        "        if layer == \"conv1\":\n",
        "            module = load_model.conv1\n",
        "        elif layer == \"conv2\":\n",
        "            module = load_model.conv2\n",
        "\n",
        "        pruneModel(module=module, dim=dim, amount=amount)\n",
        "        print(\"Pruned dim = \", dim, \", amount = \", amount)\n",
        "        accuracy_pruned = get_modelAccuracy(test_loader).data.item()\n",
        "        print(\"Accuracy for test images: %d%% --> %d%%\" % (accuracy_baseline, accuracy_pruned))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Mac2RPcJGy",
        "outputId": "f97a4283-f8ad-4150-9e4e-325858884408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruning layer:  conv2\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-30868408b8dc>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  load_model.load_state_dict(torch.load('./pretrainedModel'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned dim =  0 , amount =  0.06666666666666667\n",
            "Accuracy for test images: 86% --> 85%\n",
            "Mounted at /content/drive\n",
            "Pruned dim =  2 , amount =  0.3333333333333333\n",
            "Accuracy for test images: 86% --> 80%\n",
            "Mounted at /content/drive\n",
            "Pruned dim =  3 , amount =  0.3333333333333333\n",
            "Accuracy for test images: 86% --> 81%\n",
            "Pruning layer:  conv1\n",
            "Mounted at /content/drive\n",
            "Pruned dim =  0 , amount =  0.08333333333333333\n",
            "Accuracy for test images: 86% --> 83%\n",
            "Mounted at /content/drive\n",
            "Pruned dim =  2 , amount =  0.3333333333333333\n",
            "Accuracy for test images: 86% --> 75%\n",
            "Mounted at /content/drive\n",
            "Pruned dim =  3 , amount =  0.3333333333333333\n",
            "Accuracy for test images: 86% --> 65%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Pruned Model"
      ],
      "metadata": {
        "id": "BXdZtX1ih1M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loadModel()\n",
        "accuracy_baseline = get_modelAccuracy(test_loader).data.item()\n",
        "print(\"Initial model summary\")\n",
        "summary(load_model, (1, 28, 28))\n",
        "\n",
        "pruneModel(module=load_model.conv1, dim=0, amount=4/16)\n",
        "pruneModel(module=load_model.conv2, dim=0, amount=17/32)\n",
        "accuracy_pruned = get_modelAccuracy(test_loader).data.item()\n",
        "print(\"Accuracy for test images: %d%% --> %d%%\" % (accuracy_baseline, accuracy_pruned))\n",
        "\n",
        "prune.remove(module=load_model.conv1, name='weight')\n",
        "prune.remove(module=load_model.conv2, name='weight')\n",
        "print(\"Pruned model summary\")\n",
        "summary(load_model, (1, 28, 28))\n"
      ],
      "metadata": {
        "id": "mPfEddWkh7zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da00528b-0c1a-4048-ca44-d6a0e798791d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-1f8a9b2ac6e7>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  load_model.load_state_dict(torch.load('./pretrainedModel'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial model summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,608\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]          15,680\n",
            "================================================================\n",
            "Total params: 20,432\n",
            "Trainable params: 20,432\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n",
            "Accuracy for test images: 90% --> 86%\n",
            "Pruned model summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,608\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]          15,680\n",
            "================================================================\n",
            "Total params: 20,432\n",
            "Trainable params: 20,432\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,608\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]          15,680\n",
            "================================================================\n",
            "Total params: 20,432\n",
            "Trainable params: 20,432\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}