{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2XZFmJZvKcCOUugKyXUpH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnajakodali/ml_lab3/blob/main/mlcodsgn_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYxOkVxFN_rq",
        "outputId": "43833c5c-c0ab-455f-9e2a-dd90da838afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 9.40MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 200kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.68MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 8.89MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import easydict\n",
        "from torchsummary import summary\n",
        "\n",
        "# argument parser\n",
        "import easydict\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"lr\": 0.01,\n",
        "})\n",
        "# Hyper Parameters\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = args.epochs\n",
        "batch_size = args.batch_size\n",
        "learning_rate = args.lr\n",
        "\n",
        "# MNIST Dataset (Images and Labels)\n",
        "train_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "test_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "\n",
        "# Dataset Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_set,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_set,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConvNet(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MyConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.lin2  = nn.Linear(7*7*32, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model = MyConvNet(args)\n",
        "model = model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion=criterion.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 5e-4, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "3nRjNjA7Om-w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, (1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PNq4HpgPClw",
        "outputId": "f2b0ed2c-6538-485f-e30f-45661ba831b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,608\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]          15,680\n",
            "================================================================\n",
            "Total params: 20,432\n",
            "Trainable params: 20,432\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Training started\")\n",
        "# Training the Model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        L1norm = model.parameters()\n",
        "        arr = []\n",
        "        for name,param in model.named_parameters():\n",
        "          if 'weight' in name.split('.'):\n",
        "            arr.append(param)\n",
        "        L1loss = 0\n",
        "        for Losstmp in arr:\n",
        "          L1loss = L1loss+Losstmp.abs().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 600 == 0:\n",
        "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                    % (epoch + 1, num_epochs, i + 1,\n",
        "                       len(train_set) // batch_size, loss.data.item()))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "for images, labels in test_loader:\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    outputs = model(images)\n",
        "    testloss = criterion(outputs, labels)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print('Accuracy for test images: % d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og8QMHGHPOfY",
        "outputId": "81d7a64e-6de5-49eb-ecaf-15a8c7083a5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Training started\n",
            "Epoch: [ 1/ 10], Step: [ 600/ 1875], Loss: 0.1728\n",
            "Epoch: [ 1/ 10], Step: [ 1200/ 1875], Loss: 0.3289\n",
            "Epoch: [ 1/ 10], Step: [ 1800/ 1875], Loss: 0.1153\n",
            "Epoch: [ 2/ 10], Step: [ 600/ 1875], Loss: 0.1044\n",
            "Epoch: [ 2/ 10], Step: [ 1200/ 1875], Loss: 0.5728\n",
            "Epoch: [ 2/ 10], Step: [ 1800/ 1875], Loss: 0.2086\n",
            "Epoch: [ 3/ 10], Step: [ 600/ 1875], Loss: 0.3175\n",
            "Epoch: [ 3/ 10], Step: [ 1200/ 1875], Loss: 0.1801\n",
            "Epoch: [ 3/ 10], Step: [ 1800/ 1875], Loss: 0.4170\n",
            "Epoch: [ 4/ 10], Step: [ 600/ 1875], Loss: 0.1844\n",
            "Epoch: [ 4/ 10], Step: [ 1200/ 1875], Loss: 0.1444\n",
            "Epoch: [ 4/ 10], Step: [ 1800/ 1875], Loss: 0.3252\n",
            "Epoch: [ 5/ 10], Step: [ 600/ 1875], Loss: 0.2698\n",
            "Epoch: [ 5/ 10], Step: [ 1200/ 1875], Loss: 0.3423\n",
            "Epoch: [ 5/ 10], Step: [ 1800/ 1875], Loss: 0.3684\n",
            "Epoch: [ 6/ 10], Step: [ 600/ 1875], Loss: 0.2713\n",
            "Epoch: [ 6/ 10], Step: [ 1200/ 1875], Loss: 0.1629\n",
            "Epoch: [ 6/ 10], Step: [ 1800/ 1875], Loss: 0.3022\n",
            "Epoch: [ 7/ 10], Step: [ 600/ 1875], Loss: 0.5730\n",
            "Epoch: [ 7/ 10], Step: [ 1200/ 1875], Loss: 0.1313\n",
            "Epoch: [ 7/ 10], Step: [ 1800/ 1875], Loss: 0.2567\n",
            "Epoch: [ 8/ 10], Step: [ 600/ 1875], Loss: 0.2610\n",
            "Epoch: [ 8/ 10], Step: [ 1200/ 1875], Loss: 0.1640\n",
            "Epoch: [ 8/ 10], Step: [ 1800/ 1875], Loss: 0.4082\n",
            "Epoch: [ 9/ 10], Step: [ 600/ 1875], Loss: 0.1955\n",
            "Epoch: [ 9/ 10], Step: [ 1200/ 1875], Loss: 0.1378\n",
            "Epoch: [ 9/ 10], Step: [ 1800/ 1875], Loss: 0.1266\n",
            "Epoch: [ 10/ 10], Step: [ 600/ 1875], Loss: 0.1599\n",
            "Epoch: [ 10/ 10], Step: [ 1200/ 1875], Loss: 0.1663\n",
            "Epoch: [ 10/ 10], Step: [ 1800/ 1875], Loss: 0.1616\n",
            "Accuracy for test images:  90 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './madu_saved_lab4.pt')"
      ],
      "metadata": {
        "id": "L37xTbPPP9OP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_model = MyConvNet(args)\n",
        "load_model.load_state_dict(torch.load('./madu_saved_lab4.pt'))\n",
        "\n",
        "load_model = load_model.cuda()\n",
        "correct = 0\n",
        "total = 0\n",
        "load_model.eval()\n",
        "for images, labels in test_loader:\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    outputs = load_model(images)\n",
        "    testloss = criterion(outputs, labels)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print('Accuracy for test images: % .2f %%' % (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK1uIMHtQjkz",
        "outputId": "29252320-8c3c-43d3-b8f7-6ca794c7728b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-af1afc26a776>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  load_model.load_state_dict(torch.load('./madu_saved_lab4.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for test images:  90.32 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def USquantize(x,bits=4):\n",
        "  # Uniform symetric quantizer that\n",
        "  # quantize x into sf * qx\n",
        "  # sf: scaling factor\n",
        "  # qx: integer in range [-2^(bits-1)+1, 2^(bits-1)-1]\n",
        "  # note: only 2^bits - 1 different values can be represented, bits >= 2\n",
        "\n",
        "  max_value = torch.max(torch.abs(x))\n",
        "  sf = max_value / (2**(bits-1) - 1)\n",
        "  qx = torch.round(x/sf)\n",
        "  qx = torch.clip(qx,min=-2**(bits-1) + 1,max=2**(bits-1) - 1)\n",
        "  dqx = qx * sf\n",
        "  return dqx\n",
        "\n",
        "def UASquantize(x,bits=4):\n",
        "  # Uniform Asymetric quantizer that\n",
        "  # quantize x into min_value + sf * qx\n",
        "  # sf: scaling factor\n",
        "  # qx: integer in range [0,2^bits-1]\n",
        "  # note: 2^bits different values can be represented, bits >= 1\n",
        "\n",
        "  max_value = torch.max(x)\n",
        "  min_value = torch.min(x)\n",
        "  sf = (max_value - min_value) / (2**bits - 1)\n",
        "  qx = torch.round((x-min_value)/sf)\n",
        "  qx = torch.clip(qx,min=0,max=2**(bits) - 1)\n",
        "  dqx = min_value + qx * sf\n",
        "  return dqx\n",
        "\n",
        "def UASquantizeMinOffset(x,p,bits=4):\n",
        "  # Uniform Asymetric quantizer that\n",
        "  # quantize x into min_value + sf * qx\n",
        "  # sf: scaling factor\n",
        "  # p: percent offset away from the native min. Can be negative or positive\n",
        "  # qx: integer in range [0,2^bits-1]\n",
        "  # note: 2^bits different values can be represented, bits >= 1\n",
        "\n",
        "  max_value = torch.max(x)\n",
        "  min_value = torch.min(x)\n",
        "  # update min_value to be set by p\n",
        "  min_value = min_value * p\n",
        "  sf = (max_value - min_value) / (2**bits - 1)\n",
        "  qx = torch.round((x-min_value)/sf)\n",
        "  qx = torch.clip(qx,min=0,max=2**(bits) - 1)\n",
        "  dqx = min_value + qx * sf\n",
        "  return dqx\n",
        "\n",
        "\n",
        "def USquantize_clipped(x,bits=4,quantile=0.999):\n",
        "\n",
        "  # Uniform Symetric quantizer with clipped representation range\n",
        "  # range covers 'quantile' percent of FP32 x\n",
        "  max_value = torch.quantile(x, 1 - 0.5 * (1 - quantile))  # Upper quantile\n",
        "  min_value = torch.quantile(x, 0.5 * (1 - quantile))      # Lower quantile\n",
        "  quantile_max = max(abs(max_value), abs(min_value))  # Ensure symmetry around zero\n",
        "  sf = quantile_max / (2 ** (bits - 1) - 1)\n",
        "  qx = torch.round(x/sf)\n",
        "  qx = torch.clip(qx,min=-2**(bits-1) + 1,max=2**(bits-1) - 1)\n",
        "  dqx = qx * sf\n",
        "  return dqx\n",
        "\n",
        "def UASquantize_clipped(x,bits=4,quantile=0.999):\n",
        "\n",
        "  # Uniform Asymetric quantizer with clipped representation range\n",
        "  # range covers 'quantile' percent of FP32 x\n",
        "\n",
        "  max_value = torch.quantile(x,1-0.5*(1-quantile))\n",
        "  min_value = torch.quantile(x,0.5*(1-quantile))\n",
        "  sf = (max_value - min_value) / (2**bits - 1)\n",
        "  qx = torch.round((x-min_value)/sf)\n",
        "  qx = torch.clip(qx,min=0,max=2**(bits) - 1)\n",
        "  dqx = min_value + qx * sf\n",
        "  return dqx"
      ],
      "metadata": {
        "id": "rdRhG0i3QqRe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5EIrFKp3sINZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***SYMMETRIC SIGNED QUANTIZATION***\n",
        "\n",
        "1.   *List item*\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "kesHgqbMiC3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "q_model = deepcopy(model)\n",
        "sd = model.state_dict()\n",
        "q_sd = q_model.state_dict()\n",
        "cuda_available =torch.cuda.is_available()\n",
        "\n",
        "# weight quantization only\n",
        "# different bitwidth\n",
        "for n_bits in (4,8, 12, 16):\n",
        "  print(f'quantizing model into {n_bits} bits')\n",
        "  for name,_ in model.named_parameters():\n",
        "    # print('quantizing ',name)\n",
        "    q_sd[name] = USquantize(sd[name],bits=n_bits)\n",
        "\n",
        "  q_model.load_state_dict(q_sd)\n",
        "\n",
        "  # Test the Model\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images, labels in test_loader:\n",
        "      if cuda_available: images = images.cuda();labels = labels.cuda()\n",
        "      outputs = q_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy on the 10000 test images: % .2f %%' % (100 * correct/ total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_wtR1wHgJ7_",
        "outputId": "2ed0abd3-4d0b-4b78-9622-353d04f09608"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantizing model into 4 bits\n",
            "Accuracy on the 10000 test images:  90.10 %\n",
            "quantizing model into 8 bits\n",
            "Accuracy on the 10000 test images:  90.35 %\n",
            "quantizing model into 12 bits\n",
            "Accuracy on the 10000 test images:  90.34 %\n",
            "quantizing model into 16 bits\n",
            "Accuracy on the 10000 test images:  90.32 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ASSYMETRIC QUANTIZATION**"
      ],
      "metadata": {
        "id": "8DoK1H4Ojnq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_model = deepcopy(model)\n",
        "sd = model.state_dict()\n",
        "q_sd = q_model.state_dict()\n",
        "cuda_available =torch.cuda.is_available()\n",
        "# weight quantization only\n",
        "# different bitwidth\n",
        "for n_bits in (4,8, 12, 16):\n",
        "  print(f'quantizing model into {n_bits} bits')\n",
        "  for name,_ in model.named_parameters():\n",
        "    # print('quantizing ',name)\n",
        "    q_sd[name] = UASquantize(sd[name],bits=n_bits)\n",
        "\n",
        "  q_model.load_state_dict(q_sd)\n",
        "\n",
        "  # Test the Model\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images, labels in test_loader:\n",
        "      if cuda_available: images = images.cuda();labels = labels.cuda()\n",
        "      outputs = q_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy on the 10000 test images: % .2f %%' % (100 * correct/ total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0i3k661jsk0",
        "outputId": "0b1b3e73-4107-4b0e-90bb-194ae1a93900"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantizing model into 4 bits\n",
            "Accuracy on the 10000 test images:  90.13 %\n",
            "quantizing model into 8 bits\n",
            "Accuracy on the 10000 test images:  90.33 %\n",
            "quantizing model into 12 bits\n",
            "Accuracy on the 10000 test images:  90.34 %\n",
            "quantizing model into 16 bits\n",
            "Accuracy on the 10000 test images:  90.32 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OPTIMAL RANGE QUANTIZATION TO REMOVE OUTLIERS**\n",
        "\n",
        "symmetric"
      ],
      "metadata": {
        "id": "aXROpYJpwOL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_model = deepcopy(model)\n",
        "sd = model.state_dict()\n",
        "q_sd = q_model.state_dict()\n",
        "cuda_available =torch.cuda.is_available()\n",
        "\n",
        "# weight quantization only\n",
        "# different bitwidth\n",
        "for n_bits in (4,8, 12, 16):\n",
        "  print(f'quantizing model into {n_bits} bits')\n",
        "  for name,_ in model.named_parameters():\n",
        "    # print('quantizing ',name)\n",
        "    q_sd[name] = USquantize_clipped(sd[name],bits=n_bits)\n",
        "  q_model.load_state_dict(q_sd)\n",
        "\n",
        "  # Test the Model\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images, labels in test_loader:\n",
        "      if cuda_available: images = images.cuda();labels = labels.cuda()\n",
        "      outputs = q_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy on the 10000 test images: % .2f %%' % (100 * correct/ total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kXYGNN8wN2m",
        "outputId": "2b8eb038-8a89-476b-8489-13dfdf653965"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantizing model into 4 bits\n",
            "Accuracy on the 10000 test images:  90.06 %\n",
            "quantizing model into 8 bits\n",
            "Accuracy on the 10000 test images:  90.24 %\n",
            "quantizing model into 12 bits\n",
            "Accuracy on the 10000 test images:  90.20 %\n",
            "quantizing model into 16 bits\n",
            "Accuracy on the 10000 test images:  90.20 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OPTIMAL RANGE QUANTIZATION TO REMOVE OUTLIERS**\n",
        "\n",
        "assymetric\n",
        ":"
      ],
      "metadata": {
        "id": "rK-8vjVHjsNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_model = deepcopy(model)\n",
        "sd = model.state_dict()\n",
        "q_sd = q_model.state_dict()\n",
        "cuda_available =torch.cuda.is_available()\n",
        "\n",
        "# weight quantization only\n",
        "# different bitwidth\n",
        "for n_bits in (4,8, 12, 16):\n",
        "  print(f'quantizing model into {n_bits} bits')\n",
        "  for name,_ in model.named_parameters():\n",
        "    # print('quantizing ',name)\n",
        "    q_sd[name] = UASquantize_clipped(sd[name],bits=n_bits)\n",
        "  q_model.load_state_dict(q_sd)\n",
        "\n",
        "  # Test the Model\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images, labels in test_loader:\n",
        "      if cuda_available: images = images.cuda();labels = labels.cuda()\n",
        "      outputs = q_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy on the 10000 test images: % .2f %%' % (100 * correct/ total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BPxWVGWkbAr",
        "outputId": "fc3e09ab-9463-4039-ad4d-005515d93332"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantizing model into 4 bits\n",
            "Accuracy on the 10000 test images:  90.31 %\n",
            "quantizing model into 8 bits\n",
            "Accuracy on the 10000 test images:  90.19 %\n",
            "quantizing model into 12 bits\n",
            "Accuracy on the 10000 test images:  90.22 %\n",
            "quantizing model into 16 bits\n",
            "Accuracy on the 10000 test images:  90.22 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Per layer symmetric"
      ],
      "metadata": {
        "id": "E04xXACD48iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_model = deepcopy(model)\n",
        "sd = model.state_dict()\n",
        "q_sd = q_model.state_dict()\n",
        "\n",
        "# Weight quantization only for different bitwidths\n",
        "for n_bits in (4, 8, 12, 16):\n",
        "    print(f'Quantizing model into {n_bits}-bit weights')\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:  # Quantize only the weights, not biases\n",
        "            # Print layer being quantized\n",
        "            #print(f'Quantizing layer: {name}')\n",
        "\n",
        "            # Apply per-layer quantization (can use USquantize or per_layer_quantize function)\n",
        "            q_sd[name] = USquantize(sd[name], bits=n_bits)\n",
        "\n",
        "    # Load the quantized weights into the model\n",
        "    q_model.load_state_dict(q_sd)\n",
        "\n",
        "    # Test the Model on the test set\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "      if cuda_available: images = images.cuda();labels = labels.cuda()\n",
        "      outputs = q_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy on the 10000 test images: % .2f %%' % (100 * correct/ total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtEiPHjC48-P",
        "outputId": "ee799c30-f96e-455e-e9aa-25d2234853e8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantizing model into 4-bit weights\n",
            "Accuracy on the 10000 test images:  90.10 %\n",
            "Quantizing model into 8-bit weights\n",
            "Accuracy on the 10000 test images:  90.35 %\n",
            "Quantizing model into 12-bit weights\n",
            "Accuracy on the 10000 test images:  90.34 %\n",
            "Quantizing model into 16-bit weights\n",
            "Accuracy on the 10000 test images:  90.32 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Per layer - Assymetric"
      ],
      "metadata": {
        "id": "w8saxYFU6AZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_model = deepcopy(model)\n",
        "sd = model.state_dict()\n",
        "q_sd = q_model.state_dict()\n",
        "\n",
        "# Weight quantization only for different bitwidths\n",
        "for n_bits in (4, 8, 12, 16):\n",
        "    print(f'Quantizing model into {n_bits}-bit weights')\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:  # Quantize only the weights, not biases\n",
        "            # Print layer being quantized\n",
        "            #print(f'Quantizing layer: {name}')\n",
        "\n",
        "            # Apply per-layer quantization (can use USquantize or per_layer_quantize function)\n",
        "            q_sd[name] = UASquantize(sd[name], bits=n_bits)\n",
        "\n",
        "    # Load the quantized weights into the model\n",
        "    q_model.load_state_dict(q_sd)\n",
        "\n",
        "    # Test the Model on the test set\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "      if cuda_available: images = images.cuda();labels = labels.cuda()\n",
        "      outputs = q_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy on the 10000 test images: % .2f %%' % (100 * correct/ total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU4WMJ5D6Awe",
        "outputId": "e79c84cf-0a1d-4c8e-c567-931b20f63971"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantizing model into 4-bit weights\n",
            "Accuracy on the 10000 test images:  90.13 %\n",
            "Quantizing model into 8-bit weights\n",
            "Accuracy on the 10000 test images:  90.33 %\n",
            "Quantizing model into 12-bit weights\n",
            "Accuracy on the 10000 test images:  90.34 %\n",
            "Quantizing model into 16-bit weights\n",
            "Accuracy on the 10000 test images:  90.32 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Per layer symmetric clipped"
      ],
      "metadata": {
        "id": "_YnY6fPR6Oxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_model = deepcopy(model)\n",
        "sd = model.state_dict()\n",
        "q_sd = q_model.state_dict()\n",
        "\n",
        "# Weight quantization only for different bitwidths\n",
        "for n_bits in (4, 8, 12, 16):\n",
        "    print(f'Quantizing model into {n_bits}-bit weights')\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:  # Quantize only the weights, not biases\n",
        "            # Print layer being quantized\n",
        "            #print(f'Quantizing layer: {name}')\n",
        "\n",
        "            # Apply per-layer quantization (can use USquantize or per_layer_quantize function)\n",
        "            q_sd[name] = UASquantize_clipped(sd[name], bits=n_bits)\n",
        "\n",
        "    # Load the quantized weights into the model\n",
        "    q_model.load_state_dict(q_sd)\n",
        "\n",
        "    # Test the Model on the test set\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "      if cuda_available: images = images.cuda();labels = labels.cuda()\n",
        "      outputs = q_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy on the 10000 test images: % .2f %%' % (100 * correct/ total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF1ZOIe96P5c",
        "outputId": "d77c94f9-43af-4f8c-829d-701cfa6bc52a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantizing model into 4-bit weights\n",
            "Accuracy on the 10000 test images:  90.31 %\n",
            "Quantizing model into 8-bit weights\n",
            "Accuracy on the 10000 test images:  90.19 %\n",
            "Quantizing model into 12-bit weights\n",
            "Accuracy on the 10000 test images:  90.22 %\n",
            "Quantizing model into 16-bit weights\n",
            "Accuracy on the 10000 test images:  90.22 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Per layer asymmetric cliiped"
      ],
      "metadata": {
        "id": "aNmn7pHo6XLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_model = deepcopy(model)\n",
        "sd = model.state_dict()\n",
        "q_sd = q_model.state_dict()\n",
        "\n",
        "# Weight quantization only for different bitwidths\n",
        "for n_bits in (4, 8, 12, 16):\n",
        "    print(f'Quantizing model into {n_bits}-bit weights')\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:  # Quantize only the weights, not biases\n",
        "            # Print layer being quantized\n",
        "            #print(f'Quantizing layer: {name}')\n",
        "\n",
        "            # Apply per-layer quantization (can use USquantize or per_layer_quantize function)\n",
        "            q_sd[name] = USquantize(sd[name], bits=n_bits)\n",
        "\n",
        "    # Load the quantized weights into the model\n",
        "    q_model.load_state_dict(q_sd)\n",
        "\n",
        "    # Test the Model on the test set\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "      if cuda_available: images = images.cuda();labels = labels.cuda()\n",
        "      outputs = q_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy on the 10000 test images: % .2f %%' % (100 * correct/ total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTZytiQR6Xhy",
        "outputId": "ffa780a8-6859-4d77-cc2c-95f5b481da3a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantizing model into 4-bit weights\n",
            "Accuracy on the 10000 test images:  90.10 %\n",
            "Quantizing model into 8-bit weights\n",
            "Accuracy on the 10000 test images:  90.35 %\n",
            "Quantizing model into 12-bit weights\n",
            "Accuracy on the 10000 test images:  90.34 %\n",
            "Quantizing model into 16-bit weights\n",
            "Accuracy on the 10000 test images:  90.32 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(q_model, (1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkUuDWv6L0pR",
        "outputId": "8c61a842-c943-4486-be5c-d429a828125a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,608\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]          15,680\n",
            "================================================================\n",
            "Total params: 20,432\n",
            "Trainable params: 20,432\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation quantization"
      ],
      "metadata": {
        "id": "F_LwsetbBpRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedModel(nn.Module):\n",
        "    def __init__(self, original_model, bits=4):\n",
        "        super(QuantizedModel, self).__init__()\n",
        "        self.model = original_model\n",
        "        self.bits = bits\n",
        "\n",
        "        # Quantizing the weights of each layer\n",
        "        self.quantized_weights = {}\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                self.quantized_weights[name] = UASquantize(param, bits=self.bits)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for name, layer in self.model.named_children():\n",
        "            x = layer(x)\n",
        "            if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "                # Quantize activations after each Conv/FC layer\n",
        "                x = UASquantize(x, bits=self.bits)\n",
        "        return x\n",
        "\n",
        "q_model = QuantizedModel(model, bits=4)\n",
        "\n",
        "sd = model.state_dict()\n",
        "q_sd = q_model.state_dict()\n",
        "\n",
        "# weight quantization only\n",
        "# different bitwidth\n",
        "for n_bits in (4,8, 12, 16):\n",
        "  print(f'quantizing model into {n_bits} bits')\n",
        "  #q_model.load_state_dict(q_sd)\n",
        "  q_model.bits = n_bits\n",
        "  # Test the Model\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images, labels in test_loader:\n",
        "      if cuda_available: images = images.cuda();labels = labels.cuda()\n",
        "      outputs = q_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy on the 10000 test images: % .2f %%' % (100 * correct/ total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "HfXbO2Q9Bo7K",
        "outputId": "d60d8d20-f87d-4367-e3f0-a1e9d33414a3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantizing model into 4 bits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (7168x7 and 1568x10)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-05c49e9821a0>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcuda_available\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-05c49e9821a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;31m# Quantize activations after each Conv/FC layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (7168x7 and 1568x10)"
          ]
        }
      ]
    }
  ]
}